{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMDB Training Steps","metadata":{}},{"cell_type":"markdown","source":"## Dataset Splitting","metadata":{}},{"cell_type":"code","source":"data_folder = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\ndf = pd.read_csv(data_folder)\n\nfrom sklearn.model_selection import train_test_split\n\n# Create a mapping dictionary\nlabel_mapping = {'positive': 1, 'negative': 0}\n\n# Convert labels using the mapping dictionary\ndf['sentiment'] = df['sentiment'].map(label_mapping)\n\n# Split the data into train and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.5, random_state=1234)\n\n\n# Example usage\nprint(\"Train set size:\", len(train_df))\nprint(\"Test set size:\", len(test_df))","metadata":{"execution":{"iopub.status.busy":"2023-07-16T11:45:22.565623Z","iopub.execute_input":"2023-07-16T11:45:22.565982Z","iopub.status.idle":"2023-07-16T11:45:24.595015Z","shell.execute_reply.started":"2023-07-16T11:45:22.565952Z","shell.execute_reply":"2023-07-16T11:45:24.593970Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Train set size: 25000\nTest set size: 25000\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T11:45:24.597008Z","iopub.execute_input":"2023-07-16T11:45:24.599778Z","iopub.status.idle":"2023-07-16T11:45:24.614351Z","shell.execute_reply.started":"2023-07-16T11:45:24.599749Z","shell.execute_reply":"2023-07-16T11:45:24.613283Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                  review  sentiment\n23420  ...And that's why hard to rate. <br /><br />Fr...          0\n43821  Some have praised _Atlantis:_The_Lost_Empire_ ...          0\n21387  This film says everything there is to say abou...          1\n17127  Last time I checked, the Nazis didn't win the ...          0\n3642   I wish Depardieu had been able to finish his b...          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23420</th>\n      <td>...And that's why hard to rate. &lt;br /&gt;&lt;br /&gt;Fr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43821</th>\n      <td>Some have praised _Atlantis:_The_Lost_Empire_ ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21387</th>\n      <td>This film says everything there is to say abou...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17127</th>\n      <td>Last time I checked, the Nazis didn't win the ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3642</th>\n      <td>I wish Depardieu had been able to finish his b...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data preprocessing ","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport nltk \nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2023-07-16T11:45:26.846092Z","iopub.execute_input":"2023-07-16T11:45:26.846801Z","iopub.status.idle":"2023-07-16T11:45:27.351923Z","shell.execute_reply.started":"2023-07-16T11:45:26.846768Z","shell.execute_reply":"2023-07-16T11:45:27.350095Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nfrom string import digits\nfrom collections import Counter\nfrom torchtext.data.utils import get_tokenizer\n\n# Define the tokenizer\ntokenizer = get_tokenizer('basic_english')\n\n\ndef stringprocess(text):\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(r\"\\#\", \"\", text)\n    text = re.sub(r\"http\\S+\",\"URL\", text)\n    text = re.sub(r\"@\", \"\", text)\n    text = re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \", text)\n    text = re.sub(\"\\s{2,}\", \" \", text)\n    text = text.strip(' ')\n    text = text.lower()\n    \n    return text\n\ndef tokenprocess(text):\n    text_tokens = tokenizer(text)\n    # Filter tokens based on their frequency\n    filtered_tokens = [token for token in text_tokens if token not in stop_words]\n    return filtered_tokens\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:00:54.471162Z","iopub.execute_input":"2023-07-16T12:00:54.471532Z","iopub.status.idle":"2023-07-16T12:00:54.483120Z","shell.execute_reply.started":"2023-07-16T12:00:54.471500Z","shell.execute_reply":"2023-07-16T12:00:54.482117Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Define vocabulary from text","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\nX = df[\"review\"]\n\nX = X.apply(stringprocess)\nword_tokens = list(X.apply(tokenprocess))\n\nword_tokens_flat = [item for sublist in word_tokens for item in sublist]\n\n# Collect unique tokens from the dataset\nvocab = set()\nfor data_point in word_tokens:\n    vocab.update(data_point)\n\n# Step 1: Determine word frequencies\nword_frequency = {}\nfor word in word_tokens_flat:\n    if word in word_frequency:\n        word_frequency[word] += 1\n    else:\n        word_frequency[word] = 1\n\n# Step 2: Define threshold frequency\nthreshold = 4\n\n# Step 3: Create filtered list\nvocab = [word for word in vocab if word_frequency[word] >= threshold]\n\n# Convert the set of unique tokens to a list\nvocab = list(vocab)\nvocab = ['<pad>'] + vocab \n\nprint(len(vocab))\n\n# Example usage: Print the vocabulary\nprint(vocab[:50])\n\n# Count the number of tokens per data point\ntoken_counts = []\nfor data_point in word_tokens:\n    token_count = len(data_point)\n    token_counts.append(token_count)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:00:55.026421Z","iopub.execute_input":"2023-07-16T12:00:55.026861Z","iopub.status.idle":"2023-07-16T12:01:31.079831Z","shell.execute_reply.started":"2023-07-16T12:00:55.026831Z","shell.execute_reply":"2023-07-16T12:01:31.078640Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"44127\n['<pad>', 'dubbed', 'webcam', 'edgerton', 'incredible', 'unbelievably', 'virtual', 'coaches', 'inauspicious', 'mothering', 'hg', 'bourdelle', 'wackos', 'stamina', 'mood', 'career', 'eyecandy', 'grins', 'goffin', 'prejudiced', 'absences', 'knb', 'irreconcilable', 'rescuer', 'hushed', 'maltese', 'modes', 'wield', 'hoey', 'gulch', 'bulge', 'superdome', 'condensing', 'ij', 'fruity', 'indecisive', 'oiran', 'basing', 'nellie', 'propositions', 'gooey', 'masterful', 'cheesecake', 'neat', 'roxane', 'wobble', 'rewatched', 'loveable', 'tanisha', 'willaim']\n","output_type":"stream"}]},{"cell_type":"code","source":"len(vocab)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:01:31.081964Z","iopub.execute_input":"2023-07-16T12:01:31.082349Z","iopub.status.idle":"2023-07-16T12:01:31.089916Z","shell.execute_reply.started":"2023-07-16T12:01:31.082312Z","shell.execute_reply":"2023-07-16T12:01:31.088772Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"44127"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, df, tokenizer, vocab, max_length=500):\n        self.data = df['review']\n        self.targets = df['sentiment']\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        self.vocab_dict = {token: index for index, token in enumerate(vocab)} \n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        # Get the data and target for the given index\n        data_point = self.data.iloc[index]\n        data_point = stringprocess(data_point)\n        word_tokens = tokenprocess(data_point)\n        target = self.targets.iloc[index]\n\n        # Truncate the data point to the specified max length\n        truncated_data = word_tokens[:self.max_length]\n        data_ids = [self.vocab_dict[word] for word in truncated_data if self.vocab_dict.get(word) is not None] \n\n        return torch.tensor(data_ids), target","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:01:31.091921Z","iopub.execute_input":"2023-07-16T12:01:31.092302Z","iopub.status.idle":"2023-07-16T12:01:31.103507Z","shell.execute_reply.started":"2023-07-16T12:01:31.092268Z","shell.execute_reply":"2023-07-16T12:01:31.102555Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Using a pretrained embedding \n\nDescription of how GloVe works:\n\nGloVe (for Global Vector) takes a word and turns it into a vector. The main\nidea is that the distance between vectors that have close semantical meaning\nshould be small. Now two words can be considered to be close in some aspect\nbut very distant in some other. Consider for example the words 'men and\n'woman'. These two words can be considered close because they both describe a\nhuman being but can also be considered far from one another because they \ndescribe people of opposite sex. Therefore it seems that the right measure \nof semantical closedness should not be one-dimensional but instead multi-\ndimensional. GloVe takes that into account and consider the right notion of distance\nto be the difference between the two vectors: V(i) - V(j).\n\nLet's define a co-occurence matrix whose entry X_{ij} gives the number of times\nthe j-word appears in the context window of i. Then Sum_j X_{ij} is the number\ntimes any word apears in the context window of i. With these we can define a \nprobability P(j|i) = X_{ij}/Sum_{j} X_{ij} which is the probability of finding \nthe word j in the context window of i.\n\nNow an observation is that the P(k|i)/P(k|j) is large if i is closely related to k\nAND j is not related to k. For example, let's consider the words i = ice, \nj = steam and k = solid. Solid is related to ice but not to steam therefore in that\ncase P(k|i)/P(k|j) is large. Another example is i = ice, j = steam and k = water. In\nthat case water is both related to ice and steam and P(k|i)/P(k|j) will be of\norder one.\n\nThe goal of the training is to obtain a set of vectors that satisfy the following\nproperty:\n    V(i).transpose.V(j) = logP(i|j)\nWhy? Because then, we can compute the projection of the distance in the direction\nof the word k:\n    V(k).transpose (V(i) - V(j)) = logP(k|i)/P(k|j)\nWhen k is related to i but not to j, the argument of the log is large, when it's the \nopposite the argument is close to zero. Therefore in both those cases, the absolute\nvalue of the log is large. Now if k is related to both i and j, the argument of the log\nis close to one and the distance in that direction if small (think about the water\nexample). Note that if k = fashion, the distance in that direction will be small too\nsince fashion has nothing to do with both ice and steam.","metadata":{}},{"cell_type":"code","source":"from torchtext.vocab import GloVe\n# # Load GloVe embeddings\n# # Load a subset of GloVe embeddings\nglove = GloVe(name='6B', dim=300)\n\n# # Create a matrix to store GloVe embeddings\nembedding_matrix = np.zeros((len(vocab), 300))\n\n# # Fill the embedding matrix\nfor i, token in enumerate(vocab):\n    embedding_matrix[i] = glove[token]\n\nnp.save('embeddings.npy', embedding_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:01:31.107605Z","iopub.execute_input":"2023-07-16T12:01:31.108516Z","iopub.status.idle":"2023-07-16T12:01:33.277124Z","shell.execute_reply.started":"2023-07-16T12:01:31.108459Z","shell.execute_reply":"2023-07-16T12:01:33.276057Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.load('/kaggle/working/embeddings.npy')","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:01:33.281848Z","iopub.execute_input":"2023-07-16T12:01:33.284290Z","iopub.status.idle":"2023-07-16T12:01:33.339056Z","shell.execute_reply.started":"2023-07-16T12:01:33.284250Z","shell.execute_reply":"2023-07-16T12:01:33.337977Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport time\n\ndef collate_fn(batch):\n    # Sort the batch in descending order of input sequence lengths\n    batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n    \n    # Separate inputs and labels\n    inputs, labels = zip(*batch)\n    \n    # Get the lengths of each input sequence\n    input_lengths = [len(x) for x in inputs]\n    \n    # Pad the input sequences to the length of the longest sequence\n    padded_inputs = pad_sequence(inputs, batch_first=True)\n\n    return padded_inputs, torch.tensor(labels, dtype=torch.float32), input_lengths\n    \n\n\ntrain_dataset = CustomDataset(train_df, tokenizer, vocab)\ntest_dataset = CustomDataset(test_df, tokenizer, vocab)\n\n\n# Create a DataLoader for batching and shuffling\nbatch_size = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:01:33.343542Z","iopub.execute_input":"2023-07-16T12:01:33.346110Z","iopub.status.idle":"2023-07-16T12:01:33.413658Z","shell.execute_reply.started":"2023-07-16T12:01:33.346063Z","shell.execute_reply":"2023-07-16T12:01:33.412777Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":" vocab.index('<pad>')","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:01:33.418163Z","iopub.execute_input":"2023-07-16T12:01:33.419119Z","iopub.status.idle":"2023-07-16T12:01:33.429313Z","shell.execute_reply.started":"2023-07-16T12:01:33.419082Z","shell.execute_reply":"2023-07-16T12:01:33.427866Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nsentence, label, seq_lengths = batch\nprint(label)\n# sentence = torch.tensor(sentence)\nprint(sentence.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:01:33.431943Z","iopub.execute_input":"2023-07-16T12:01:33.433819Z","iopub.status.idle":"2023-07-16T12:01:33.486341Z","shell.execute_reply.started":"2023-07-16T12:01:33.433785Z","shell.execute_reply":"2023-07-16T12:01:33.485484Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"tensor([1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.])\ntorch.Size([32, 320])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Using LSTM\n\nSince the IMDb dataset is not very big, we can use text representations that were pretrained on large-scale corpora to reduce overfitting. We will represent each otken using the pretrained GloVe model, and feed these token representations into a multilayer bidirectional LSTM to obtain the text sequence representation, which will be transformed into sentiment analysis outputs. \n\nIn text classification tasks, a varying-length text sequence will be transformed into fixed-length categories. In the following BiLSTM class, while each token of a text sequence gets its individual pretrained GloVe representation via the embedding layer (`self.embedding`, the entire sequence is encoded by a directional LSTM (`self.encoder`). More concretely, the hidden states (at the last layer) of the bidirectional LSTM at both the initial and final time steps are concatenated as the representation of the text sequence. This single text representation is then transformed into output categories by a fully connected layer (`self.decoder`) with two outputs (\"positive\" and \"negative\").","metadata":{}},{"cell_type":"code","source":"def masked_softmax(attn_odds, masks) :\n    attentions = torch.softmax(F.relu(attn_odds.squeeze()), dim=-1)\n    # create mask based on the sentence lengths\n   \n    # apply mask and renormalize attention scores (weights)\n    masked = attn_odds * masks\n    _sums = masked.sum(-1).unsqueeze(-1)  # sums per row\n\n    attn_odds = masked.div(_sums)\n    return attn_odds","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:25:03.495661Z","iopub.execute_input":"2023-07-16T12:25:03.496022Z","iopub.status.idle":"2023-07-16T12:25:03.502559Z","shell.execute_reply.started":"2023-07-16T12:25:03.495991Z","shell.execute_reply":"2023-07-16T12:25:03.501593Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:01:41.206319Z","iopub.execute_input":"2023-07-16T12:01:41.206706Z","iopub.status.idle":"2023-07-16T12:01:41.211521Z","shell.execute_reply.started":"2023-07-16T12:01:41.206674Z","shell.execute_reply":"2023-07-16T12:01:41.210596Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class TanhAttention(nn.Module):\n    def __init__(self, hidden_size) :\n        super().__init__()\n        self.attn1 = nn.Linear(hidden_size, hidden_size // 2)\n        self.attn2 = nn.Linear(hidden_size // 2, 1, bias=False)\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    def forward(self, hidden, lengths):\n        #input_seq = (B, L), hidden : (B, L, H), masks : (B, L)\n        max_len = hidden.shape[1]\n        attn1 = nn.Tanh()(self.attn1(hidden))\n        attn2 = self.attn2(attn1).squeeze(-1)\n        masks = torch.ones(attn2.size(), requires_grad=False).to(self.device)\n        for i, l in enumerate(lengths):  # skip the first sentence\n            if l < max_len:\n                masks[i, l:] = 0\n        \n                \n        attn = masked_softmax(attn2, masks)\n        # apply attention weights\n        weighted = torch.mul(hidden, attn.unsqueeze(-1).expand_as(hidden))\n\n        # get the final fixed vector representations of the sentences\n        representations = weighted.sum(1).squeeze()\n\n        return representations, attn","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:26:48.361092Z","iopub.execute_input":"2023-07-16T12:26:48.361438Z","iopub.status.idle":"2023-07-16T12:26:48.371288Z","shell.execute_reply.started":"2023-07-16T12:26:48.361407Z","shell.execute_reply":"2023-07-16T12:26:48.370264Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nclass AttentionLSTM(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        emb_dim,\n        hidden_size,\n        num_classes,\n        dropout = 0.4,\n        lstm_layer = 2\n\n    ):\n        super(AttentionLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.lstm = nn.LSTM(input_size = emb_dim, hidden_size = hidden_size, bidirectional = True)\n        self.attention = TanhAttention(hidden_size = hidden_size*2)\n        self.fc1 = nn.Sequential(nn.Linear(hidden_size*lstm_layer, hidden_size*lstm_layer),\n                                 nn.BatchNorm1d(hidden_size*lstm_layer),\n                                 nn.ReLU())\n        self.fc2 = nn.Linear(hidden_size*lstm_layer, num_classes)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, x_len):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n        out1, (h_n, c_n) = self.lstm(x)\n        x, lengths = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)\n        x, _ = self.attention(x, lengths) # skip connect\n\n\n        y = self.fc1(self.dropout(x))\n        y = self.fc2(self.dropout(y))\n        y = self.sigmoid(y.squeeze())\n        return y\n\n    def atten_forward(self, x, x_len):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        x = nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n        out1, (h_n, c_n) = self.lstm(x)\n        x, lengths = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)\n        x, _ = self.attention(x, lengths) # skip connect\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:26:49.367546Z","iopub.execute_input":"2023-07-16T12:26:49.367953Z","iopub.status.idle":"2023-07-16T12:26:49.391599Z","shell.execute_reply.started":"2023-07-16T12:26:49.367921Z","shell.execute_reply":"2023-07-16T12:26:49.390555Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"embed_size, num_hiddens, num_layers, device = 300, 128, 1, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnet = AttentionLSTM(\n        vocab_size = len(vocab), \n        emb_dim = embed_size,\n        hidden_size = num_hiddens,\n        num_classes = 1,\n        dropout = 0.4,\n)\n\nnet.to(device)\n\ndef init_weights(module):\n    if type(module) == nn.Linear:\n        nn.init.xavier_uniform_(module.weight)\n    if type(module) == nn.LSTM:\n        for param in module._flat_weights_names:\n            if \"weight\" in param:\n                nn.init.xavier_uniform_(module._parameters[param])\nnet.apply(init_weights)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:26:51.285484Z","iopub.execute_input":"2023-07-16T12:26:51.286308Z","iopub.status.idle":"2023-07-16T12:26:51.447533Z","shell.execute_reply.started":"2023-07-16T12:26:51.286266Z","shell.execute_reply":"2023-07-16T12:26:51.446595Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"AttentionLSTM(\n  (embedding): Embedding(44127, 300)\n  (dropout): Dropout(p=0.4, inplace=False)\n  (lstm): LSTM(300, 128, bidirectional=True)\n  (attention): TanhAttention(\n    (attn1): Linear(in_features=256, out_features=128, bias=True)\n    (attn2): Linear(in_features=128, out_features=1, bias=False)\n  )\n  (fc1): Sequential(\n    (0): Linear(in_features=256, out_features=256, bias=True)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n  )\n  (fc2): Linear(in_features=256, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"Load GloVe embeddings ","metadata":{}},{"cell_type":"code","source":"net.embedding.weight.data.copy_(torch.tensor(embedding_matrix).to(device))\nnet.embedding.weight.requires_grad = False\n\nlr, num_epochs = 0.01, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:26:56.540850Z","iopub.execute_input":"2023-07-16T12:26:56.541210Z","iopub.status.idle":"2023-07-16T12:26:56.651903Z","shell.execute_reply.started":"2023-07-16T12:26:56.541180Z","shell.execute_reply":"2023-07-16T12:26:56.650852Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom torch.nn import functional as F\n\ndef compute_acc(preds, labels):\n    correct = sum((preds>0.5) == labels)\n    acc = float(correct) / float(len(labels.data)) * 100.0\n    return acc","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:27:02.016250Z","iopub.execute_input":"2023-07-16T12:27:02.017253Z","iopub.status.idle":"2023-07-16T12:27:02.023680Z","shell.execute_reply.started":"2023-07-16T12:27:02.017219Z","shell.execute_reply":"2023-07-16T12:27:02.022652Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:27:04.396539Z","iopub.execute_input":"2023-07-16T12:27:04.396916Z","iopub.status.idle":"2023-07-16T12:27:04.402706Z","shell.execute_reply.started":"2023-07-16T12:27:04.396888Z","shell.execute_reply":"2023-07-16T12:27:04.401610Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"next(net.parameters()).is_cuda","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:25:38.770685Z","iopub.execute_input":"2023-07-16T12:25:38.771635Z","iopub.status.idle":"2023-07-16T12:25:38.780610Z","shell.execute_reply.started":"2023-07-16T12:25:38.771587Z","shell.execute_reply":"2023-07-16T12:25:38.779622Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train and Eval","metadata":{}},{"cell_type":"code","source":"def train(model, optimizer, num_epochs, train_dataloader, val_dataloader, device, loss, show_every, Bert=False):\n    \n    train_losses = []\n    train_accuracies = []\n    val_losses = []\n    val_accuracies = []\n\n\n    # For each epoch...\n    for epoch_i in range(0, num_epochs):\n\n        store_train_loss = []\n        store_train_acc = []\n        store_val_loss = []\n        store_val_acc = []\n\n         # ========================================\n        #               Training\n        # ========================================\n\n        # Perform one full pass over the training set.\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_epochs))\n        print('Training...')\n        # Measure how long the training epoch takes.\n        t0 = time.time()\n        # Reset the total loss for this epoch.\n        total_loss = 0\n\n        model.train()\n    \n\n        for i, batch in enumerate(train_dataloader):\n            \n            # Add batch to GPU\n            temp_batch = []\n            for t in batch:\n                if isinstance(t, torch.Tensor):\n                    t = t.to(device)\n                temp_batch.append(t)\n            batch = temp_batch\n            if Bert: # if we're using the Bert model, see later \n                inputs_ids = batch[0]\n                attention_masks = batch[1]\n                labels = batch[2].squeeze()\n            else:\n                inputs_ids = batch[0]\n                labels = batch[1].squeeze()\n                seq_lengths = batch[2]\n            \n            optimizer.zero_grad()\n            \n            if Bert:\n                # Perform a forward pass (evaluate the model on this training batch).\n                outputs = model(inputs_ids, \n                            attention_mask=attention_masks)[0].squeeze()\n            else:\n                outputs = model(inputs_ids, seq_lengths)\n                #print(outputs)\n            outputs = outputs.squeeze()\n            train_loss = loss(outputs, labels)\n            train_acc = compute_acc(outputs, labels)\n            \n            store_train_loss.append(train_loss.item())\n            store_train_acc.append(train_acc)\n        \n            train_loss.backward()\n            optimizer.step()\n        \n            # Progress update every x batches.\n            if i % show_every == 0 and not i == 0:\n                # Calculate elapsed time in minutes.\n                elapsed = format_time(time.time() - t0)\n                # Report progress.\n                print('  Batch {} / {}.'.format(i, len(train_dataloader)))\n                print('Training loss: %.3f  Training acc: %.3f'%(np.mean(store_train_loss[-show_every:]), np.mean(store_train_acc[-show_every:])) ) \n                \n        # compute epoch loss and accuracy \n        train_losses.append(np.mean(store_train_loss))\n        train_accuracies.append(np.mean(store_train_acc))\n\n        print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n\n\n\n        # ========================================\n        #               Validation\n        # ========================================\n        # After the completion of each training epoch, measure our performance on\n        # our validation set.\n\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n\n\n        # Put the model in evaluation mode--the dropout layers behave differently\n        # during evaluation.\n        model.eval()\n\n        # Evaluate data for one epoch\n        for batch in val_dataloader:\n\n            # Add batch to GPU\n            temp_batch = []\n            for t in batch:\n                if isinstance(t, torch.Tensor):\n                    t = t.to(device)\n                temp_batch.append(t)\n            batch = temp_batch\n                \n            if Bert: # if we're using the Bert model, see later \n                inputs_ids = batch[0]\n                attention_masks = batch[1]\n                labels = batch[2].squeeze()\n            else:\n                inputs_ids = batch[0]\n                labels = batch[1].squeeze()\n                seq_lengths = batch[2]\n            with torch.no_grad():\n\n                if Bert:\n                    # Perform a forward pass (evaluate the model on this training batch).\n                    outputs = model(inputs_ids, \n                                attention_mask=attention_masks)[0].squeeze()\n                else:\n                    outputs = model(inputs_ids, seq_lengths)\n            \n                \n            outputs = outputs.squeeze()    \n            val_loss = loss(outputs, labels)\n            val_acc = compute_acc(outputs, labels)\n\n            store_val_loss.append(val_loss.item())\n            store_val_acc.append(val_acc)\n            \n\n        # compute epoch loss and accuracy \n        mean_val_loss = np.mean(store_val_loss)\n        val_losses.append(mean_val_loss)\n        val_accuracies.append(np.mean(store_val_acc))\n\n        # Report the final accuracy for this validation run.\n        # Print loss and acc at the end of the epoch\n        print(\"Epoch {}: Train Loss: {:.4f}, Validation Loss: {:.4f}, Train Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%\".format\n        (epoch_i+1, train_losses[-1], val_losses[-1], train_accuracies[-1], val_accuracies[-1]))\n        \n    return train_losses, val_losses, train_accuracies, val_accuracies\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:27:08.080628Z","iopub.execute_input":"2023-07-16T12:27:08.080990Z","iopub.status.idle":"2023-07-16T12:27:08.104091Z","shell.execute_reply.started":"2023-07-16T12:27:08.080961Z","shell.execute_reply":"2023-07-16T12:27:08.103065Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"train_losses, test_losses, train_accuracies, test_accuracies = train(net, optimizer, 8, train_dataloader, \n                                          test_dataloader, device, loss, show_every=200, Bert=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:27:08.896402Z","iopub.execute_input":"2023-07-16T12:27:08.897414Z","iopub.status.idle":"2023-07-16T12:36:42.970162Z","shell.execute_reply.started":"2023-07-16T12:27:08.897368Z","shell.execute_reply":"2023-07-16T12:36:42.969128Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 8 ========\nTraining...\n  Batch 200 / 782.\nTraining loss: 0.591  Training acc: 70.594\n  Batch 400 / 782.\nTraining loss: 0.418  Training acc: 81.922\n  Batch 600 / 782.\nTraining loss: 0.473  Training acc: 77.797\n  Training epoch took: 0:00:43\n\nRunning Validation...\nEpoch 1: Train Loss: 0.4769, Validation Loss: 0.3589, Train Accuracy: 77.96%, Validation Accuracy: 84.84%\n\n======== Epoch 2 / 8 ========\nTraining...\n  Batch 200 / 782.\nTraining loss: 0.395  Training acc: 82.781\n  Batch 400 / 782.\nTraining loss: 0.384  Training acc: 84.094\n  Batch 600 / 782.\nTraining loss: 0.402  Training acc: 82.984\n  Training epoch took: 0:00:43\n\nRunning Validation...\nEpoch 2: Train Loss: 0.3861, Validation Loss: 0.3268, Train Accuracy: 83.60%, Validation Accuracy: 86.19%\n\n======== Epoch 3 / 8 ========\nTraining...\n  Batch 200 / 782.\nTraining loss: 0.357  Training acc: 84.891\n  Batch 400 / 782.\nTraining loss: 0.366  Training acc: 84.672\n  Batch 600 / 782.\nTraining loss: 0.352  Training acc: 85.047\n  Training epoch took: 0:00:42\n\nRunning Validation...\nEpoch 3: Train Loss: 0.3558, Validation Loss: 0.3174, Train Accuracy: 84.97%, Validation Accuracy: 86.29%\n\n======== Epoch 4 / 8 ========\nTraining...\n  Batch 200 / 782.\nTraining loss: 0.328  Training acc: 86.375\n  Batch 400 / 782.\nTraining loss: 0.316  Training acc: 87.031\n  Batch 600 / 782.\nTraining loss: 0.338  Training acc: 85.844\n  Training epoch took: 0:00:42\n\nRunning Validation...\nEpoch 4: Train Loss: 0.3300, Validation Loss: 0.2939, Train Accuracy: 86.26%, Validation Accuracy: 87.55%\n\n======== Epoch 5 / 8 ========\nTraining...\n  Batch 200 / 782.\nTraining loss: 0.319  Training acc: 86.953\n  Batch 400 / 782.\nTraining loss: 0.363  Training acc: 84.484\n  Batch 600 / 782.\nTraining loss: 0.337  Training acc: 85.422\n  Training epoch took: 0:00:43\n\nRunning Validation...\nEpoch 5: Train Loss: 0.3365, Validation Loss: 0.2973, Train Accuracy: 85.73%, Validation Accuracy: 87.33%\n\n======== Epoch 6 / 8 ========\nTraining...\n  Batch 200 / 782.\nTraining loss: 0.332  Training acc: 86.500\n  Batch 400 / 782.\nTraining loss: 0.308  Training acc: 87.594\n  Batch 600 / 782.\nTraining loss: 0.314  Training acc: 87.672\n  Training epoch took: 0:00:42\n\nRunning Validation...\nEpoch 6: Train Loss: 0.3170, Validation Loss: 0.2805, Train Accuracy: 87.25%, Validation Accuracy: 88.24%\n\n======== Epoch 7 / 8 ========\nTraining...\n  Batch 200 / 782.\nTraining loss: 0.307  Training acc: 87.422\n  Batch 400 / 782.\nTraining loss: 0.306  Training acc: 87.562\n  Batch 600 / 782.\nTraining loss: 0.307  Training acc: 87.328\n  Training epoch took: 0:00:42\n\nRunning Validation...\nEpoch 7: Train Loss: 0.3066, Validation Loss: 0.2885, Train Accuracy: 87.43%, Validation Accuracy: 87.94%\n\n======== Epoch 8 / 8 ========\nTraining...\n  Batch 200 / 782.\nTraining loss: 0.303  Training acc: 87.719\n  Batch 400 / 782.\nTraining loss: 0.338  Training acc: 86.844\n  Batch 600 / 782.\nTraining loss: 0.330  Training acc: 86.297\n  Training epoch took: 0:00:42\n\nRunning Validation...\nEpoch 8: Train Loss: 0.3285, Validation Loss: 0.2947, Train Accuracy: 86.66%, Validation Accuracy: 87.52%\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(net.state_dict(), 'imdb_bilstm_tanh_attention_glove_300d.pt')","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:45:30.182373Z","iopub.execute_input":"2023-07-16T12:45:30.182753Z","iopub.status.idle":"2023-07-16T12:45:30.302539Z","shell.execute_reply.started":"2023-07-16T12:45:30.182722Z","shell.execute_reply":"2023-07-16T12:45:30.301556Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_loss_acc(train_losses, val_losses, train_accuracies, val_accuracies):\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].plot(train_losses, label='Train Loss')\n    axs[0].plot(val_losses, label='Validation Loss')\n    axs[0].set_title(\"Losses over Epochs\")\n    axs[0].set_xlabel(\"Epoch\")\n    axs[0].set_ylabel(\"Loss\")\n    axs[0].legend()\n    \n    axs[1].plot(train_accuracies, label='Train Accuracy')\n    axs[1].plot(val_accuracies, label='Validation Accuracy')\n    axs[1].set_title(\"Accuracies over Epochs\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].set_ylabel(\"Accuracy\")\n    axs[1].set_ylim((50,100))\n    axs[1].legend()\n    \n    plt.tight_layout()\n#     plt.savefig(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss_acc(train_losses, test_losses, train_accuracies, test_accuracies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def predict_sentiment(net, vocab, sequence):\n#     sequence = tokenizer(sequence)\n#     vocab_dict = {token: index for index, token in enumerate(vocab)}\n#     sequence = [vocab_dict[word] for word in sequence] \n    \n#     sequence = torch.tensor([sequence], device=device)\n#     output = net(sequence)\n#     return 'positive' if output > 0.5 else 'negative'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict_sentiment(net, vocab, 'I had a great time')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}